{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adbff74c-bad7-48db-8c06-46c8ae5d06e7",
   "metadata": {},
   "source": [
    "# Tutorial 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03f544f-0f47-48c8-97c2-0a3122652135",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382b72f0-283f-428b-bc77-55403b690607",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6e5406-2ec0-42b3-bcef-8c3856cfb7ac",
   "metadata": {},
   "source": [
    "Welcome! This tutorial will show you how to visualise and preprocess astronomical data using python. From this tutorial, you will learn the following:\n",
    "\n",
    "1. How to download astronomical data\n",
    "2. How to read and visualise data\n",
    "3. How to preprocess imagery data\n",
    "\n",
    "The data [GalaxyMNIST](https://github.com/mwalmsley/galaxy_mnist) contains 10,000 images of galaxies (either 3x64x64 or 3x224x224), labelled by Galaxy Zoo volunteers as belonging to one of four morphology classes, where the classes are:\n",
    "\n",
    "0. smooth and round\n",
    "1. smooth and cigar-shaped\n",
    "2. edge-on-disk\n",
    "3. unbarred spiral\n",
    "\n",
    "The galaxies are selected from `Galaxy Zoo DECaLS Campaign` A (GZD-A). The images are shown to volunteers on Galaxy Zoo for them to classify. For each image, at least 17 people were asked the necessary questions, and at least half of them must have answered with the given class. The class labels are, therefore, much more confident than, for example, simply labelling with the most common answer to some question. For more info, visit this [link](https://github.com/mwalmsley/galaxy_mnist)\n",
    "\n",
    "\n",
    "Labelling data is a big topic in machine learning. Labelled data allow us to use supervised machine learning tools to train on labelled data and classify unlabeled ones. However, labelling data can be costly. Scientists have been trying to figure out a way to label data, but the easiest is that we can depend on human knowledge to label data like the `Galaxy Zoo DECaLS Campaign`.  However, unlabeled data can also be of some benefit if analyzed with the right tools, like unsupervised machine learning. Those tools allow us to explore (find patterns) and cluster the data, leading to discoveries in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9861171f-09cb-4087-8a38-c5efdcc626b6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e74ab73-1be1-4063-b552-dc26466d6390",
   "metadata": {},
   "source": [
    "### Downloading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7505e73e-8fe3-4f89-9299-168c52974f4a",
   "metadata": {},
   "source": [
    "First, let us call the python script that will let us download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b7a79-6210-47e9-a88e-050230bae66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/mwalmsley/galaxy_mnist\n",
    "from galaxy_mnist import GalaxyMNISTHighrez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0a8f9b-705a-4289-a0a3-5f953f3bcad6",
   "metadata": {},
   "source": [
    "'GalaxyMNISTHighrez' class has the following attributes: \n",
    " - `root`: Specifing the target directory to download the data\n",
    " - `download:` A boolean value, `True` to download the images\n",
    " - `train:` A boolean value, `True` to download just the training data, `False` will download just the testing data.\n",
    "   - `Notice:` The data has a fixed 80/20 train/test division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e526b1-fa23-4b61-be7f-e0b69f2fe157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the training data\n",
    "dataset_train = GalaxyMNISTHighrez(\n",
    "    root='data_import/data',\n",
    "    download=True,\n",
    "    train=True  # by default True, or False for canonical test set\n",
    ")\n",
    "# for the testing data\n",
    "dataset_test = GalaxyMNISTHighrez(\n",
    "    root='data_import/data',\n",
    "    download=True,\n",
    "    train=False  # by default True, or False for canonical test set\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c67448b-a84d-4d3b-8c20-4be1c8707f81",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0811f16c-41f5-4576-8346-489b81daeaf5",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8bd2ee-cda3-4eea-a685-45ea0b6ebd7c",
   "metadata": {},
   "source": [
    "First, let's read the inputs `images` and the outputs `labels` into two seperate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1681f5-5382-4179-b0f1-b8932018c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train = dataset_train.data\n",
    "labels_train = dataset_train.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc89ec2-cfed-4756-b595-4ab69eb1bb56",
   "metadata": {},
   "source": [
    "Now check the shapes and data types to see they are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d453cced-95bd-498a-acc8-83da58636e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the input (training): \"+str(images_train.shape)+ \", type: \"+ str(images_train.dtype))\n",
    "print(\"Shape of the output (training): \"+str(labels_train.shape)+ \", type: \"+ str(labels_train.dtype))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9453e0f1-96c5-4fc6-8d7c-98447cf9893d",
   "metadata": {},
   "source": [
    "We notice the following:\n",
    " 1. The input data has 8000 samples with 3 channels, which stand for `rgb` colors, where each sample has a size of 224 x 224.\n",
    " 2. The output is a one-dimensional array which contains the labels of the samples. Each label is 0,1,2,3 according to the categories listed in the first cell above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52a7bbd-b5d3-4fe3-b8f2-6d62d667dacb",
   "metadata": {},
   "source": [
    "**Exercise 1:**\n",
    "Store the testing set inputs and outputs into two different variables, and display their shapes and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a886e8a-9fab-4d24-8011-c74372955262",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Code here --\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3047b3ef-da99-4da4-9aef-34fdb1fad04b",
   "metadata": {},
   "source": [
    "Now, let's check the frequencies of the labels/output. First we display the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4970e7-3b81-4593-8e82-3f203ad94b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "GalaxyMNISTHighrez.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6030ce59-c41b-4509-a591-9033e2035216",
   "metadata": {},
   "source": [
    "   Where (as indicated in the first cell):\n",
    "   - 0: `smooth_round`,\n",
    "   - 1: `smooth_cigar`,\n",
    "   - 2: `edge_on_disk`,\n",
    "   - 3: `unbarred_spiral`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b844c0d0",
   "metadata": {},
   "source": [
    "Next use numpy functions to summarize the number in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8112e9c-3754-4287-b5a4-659fdc871f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # For handling N-DIMENSIONAL ARRAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa1f9c0-d4ee-4932-9b03-bc30615fec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_train)\n",
    "labels_trainNP = np.array(labels_train)\n",
    "unique_labels, count_labels = np.unique(labels_trainNP, return_counts=True)\n",
    "print(\"Label, count:\", unique_labels, count_labels )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ac782-2c19-4550-83f8-12932b3be7c3",
   "metadata": {},
   "source": [
    "### Data visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2980958-3242-42f5-93f4-9a02128c1a33",
   "metadata": {},
   "source": [
    "We can also try to visualise label frequencies in a bar graph. To do this, we need the `pyplot' graphics package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d860a861-e2d8-41ee-a73f-a62a5df5f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt #This displays graphs once they have been created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ceeebe",
   "metadata": {},
   "source": [
    "The code for plotting is below. To make a clear display, we replace class labels with the actual class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a88dc3-0f41-4cbf-81b8-73fd13cb5f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(unique_labels)), count_labels, align='center')\n",
    "plt.xticks(range(len(unique_labels)), GalaxyMNISTHighrez.classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b030ef-042b-4dc4-aa0b-78ccb6f84a96",
   "metadata": {},
   "source": [
    "**Exercise 2:**\n",
    "Count the class frequencies for the testing data, and visualise using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f80ca70-7f59-4f2e-803a-6dbf2f45039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -- Code here --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2437829e-5fa1-4096-b53f-7f93afa15d97",
   "metadata": {},
   "source": [
    "Now let us take a look at the images of the different galaxy shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8e085-fbe0-485e-8826-00f84ed53a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image #  PIL is the Python Imaging Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6dc8f-2107-4c28-8297-4f71c46f8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display nImage images for each class\n",
    "nImage = 5\n",
    "# Loop through classes\n",
    "for j in range(len(GalaxyMNISTHighrez.classes)):\n",
    "    fig = plt.figure(figsize=(8, 8))# Figure is 8 inches by 8 inches\n",
    "    for i in range (nImage):    # Create images in each column\n",
    "        train_image = images_train[(labels_train == j)][i]\n",
    "        fig.add_subplot(1, nImage, i+1)\n",
    "        plt.imshow(np.array(train_image).transpose(1, 2, 0))\n",
    "    print(\"label: \"+str(GalaxyMNISTHighrez.classes[j]))\n",
    "    plt.tight_layout()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff98eb7-d589-4c01-a8d2-c6e4f7cb08bc",
   "metadata": {},
   "source": [
    "The resolution between classes seems unclear. For instance, `smooth cigar` and `edge_on_disk` look like each other. this will decrease accuracy when performing classification or clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef633f20-c7fe-456a-bca4-e0f63a07e1fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a2e4fd-e10a-473c-a754-26fe1859a148",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a5fbea-1d8f-4983-898d-f610a09d60b8",
   "metadata": {},
   "source": [
    "Classical machine learning often subjects the data to extensive preprocessing before classification, especially when the datasets involved are relatively small. Preprocessing functions include feature extraction, optimization, normalization, augmentation, cropping, resizing, denoising, and so on. For each of these functions, there are many alternative algorithms. \n",
    "\n",
    "On the other hand, deep learning models with lots of data are often referred to as End-to-End learning models ([E2E](https://towardsdatascience.com/e2e-the-every-purpose-ml-method-5d4f20dafee4)), since they can extract information from raw data.\n",
    "\n",
    "In the following tutorial, we will use an [_autoencoder_](https://en.wikipedia.org/wiki/Autoencoder), which uses deep learning to extract features. In theory, we can feed the raw data into the autoencoder without any preprocessing. However, in practice this greatly increases the computation time required. In this tutorial series we need to train the models in a reasonable time so that we can bring the idea across. The issue is especially important because optimal use of raw data requires hyperparameter tuning, which is even more time consuming.\n",
    "\n",
    "However, preprocessing has a caveat. We need to make sure that the preprocessing does not lose any useful information. How can we verify this? In our case, since the data is imagery data we can examine the preprocessed images visually. If we can still easily classify them, then we can conclude that the preprocessing is effective.\n",
    "\n",
    "Using an autoencoder is just one possibility. Participants are encouraged to explore alternatives.\n",
    "\n",
    "In this tutorial, we will try to reduce the complexity of the data by doing the following:\n",
    "   - Convert images to grayscale (reduces data size by a factor  of 3)\n",
    "   - Reduce the size of the image by a factor of 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5933e22-fe04-40de-8354-ee15f34bbc96",
   "metadata": {},
   "source": [
    "#### A. Grayscaling\n",
    "We begin with conversion to grayscale, following this [tutorial](https://www.tutorialspoint.com/pytorch-how-to-convert-an-image-to-grayscale) from [tutorialspoint.com](https://www.tutorialspoint.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00708585-0753-43c2-b500-6320e25f493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms #Transforms are for common image transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f713795-caae-463e-a2d7-02baf84f32c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shape of image data\n",
    "images_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16100fa-e332-4a32-937d-cfd3b7f8db06",
   "metadata": {},
   "source": [
    "We can see that the original image size has three channels which stand for the RGB colours. Now let us perfom the grayscalling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37cd306-8d90-4a3e-8777-e20ed2c42bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformGrey = transforms.Grayscale()\n",
    "images_trainGrey = transformGrey(images_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d97ed52-62c4-4b8f-aa95-59182b7cd51d",
   "metadata": {},
   "source": [
    "Now let us check the size of the resulting tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c518854-5fb5-4993-9dc7-2a2a0700bfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_trainGrey.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da834b13-6b0c-4b02-9972-c8f97430cab7",
   "metadata": {},
   "source": [
    "we can see the number of channels went down from 3 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f92a8b-3463-42d0-bbca-8ebe55a9c163",
   "metadata": {},
   "source": [
    "**Exercise 3:**\n",
    "Display a set grayscale images for each classes (like the display of images above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ef7e7e-7b0b-4a95-b45e-4b7fda8c90dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### -- Code here__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e0c702-ec9a-4838-97f2-dbd560db0e2c",
   "metadata": {},
   "source": [
    "We can notice that visually that nothing changed much from the original data. However, we reduced the complexity of the data by a favour of 3, which is enormous!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f23918-79d1-477b-9e63-0ed5dda9b2b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### B. Image resizing  \n",
    "(see  [this reference](https://www.tutorialspoint.com/pytorch-how-to-resize-an-image-to-a-given-size))\n",
    "\n",
    "Reducing the size of the image can further reduce the complexity of the image data. However, we must be careful not to reduce so much as to removed distinctions between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e01eeee-03d1-4b21-9726-ad7fddc4ebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transform. The number in parenthesis gives number of pixels per side.\n",
    "transformResize = transforms.Resize(56)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591387e-b384-4c44-b293-65cacec4c41f",
   "metadata": {},
   "source": [
    "Now let's reduce the grayscale images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e16e25-1fc3-4931-8245-38a6a248567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_trainResized = transformResize(images_trainGrey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63563091-5140-45d5-aad3-f5933ba6dd14",
   "metadata": {},
   "source": [
    "Check the size again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27581f1b-e728-4e07-b6e3-eda072969701",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_trainResized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96c7abc-b630-4fb7-ba86-4d06c67e9119",
   "metadata": {},
   "source": [
    "**Exercise 4:**\n",
    "Visualise the resized images for all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6228e9-9d8e-49d3-9760-b9794393590f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#___ Code here___\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ce2ec6-c0b5-485f-91ab-da9442e6d4fc",
   "metadata": {},
   "source": [
    "##### **_End of the pre-processing pipeline_**\n",
    "\n",
    "Now the above pre-processing is not the only way where we can reduce the features, the following can also be done:\n",
    "1. Cropping images away from the centre\n",
    "2. Image segmentation, maybe to remove background noise (Deep learning can be used for this)\n",
    "3. Explore ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5773d3-aa06-490c-93f2-c5ca52476980",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c35d031-dc9a-4642-ab6c-99f4a134adb9",
   "metadata": {},
   "source": [
    "The pixel levels run from 0 to 255. If we divide by 255, we will put the levels within a standard scale of 0 to 1. Then they are suitable to use with other features that are similarly normalised. ([source](https://developers.google.com/machine-learning/data-prep/transform/normalization))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64f75cf-e7a0-4550-93b8-169fbab9d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_trainPre = images_trainResized/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885b96e0-2c4d-4848-96e4-b4bfbc25a3cd",
   "metadata": {},
   "source": [
    "Now let us check the values for one image before and after the nomalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5699e7f-c2e7-484d-b68b-d638735dbc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pixel levels before normalization:\")\n",
    "print(images_trainResized[0][0][0])\n",
    "print(\"Pixel levels after  normalization:\")\n",
    "print(images_trainPre[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f4c8b-4269-4f37-93eb-2d995c246274",
   "metadata": {},
   "source": [
    "**Exercise 5**: Perform the preprocessing pipeline but on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6238447f-95bc-4c42-812b-32ba8f51b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#___ Code here___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dff1771-5ae0-4903-893b-258a96ea2b9f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49edf65d-e9ca-497a-a0fa-e4932faa1d1f",
   "metadata": {},
   "source": [
    "### **_End of Tutorial 1_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1fa6d6-5302-4cf5-a04d-5a45d2ba974a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GalGal_ker",
   "language": "python",
   "name": "galgal_ker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
